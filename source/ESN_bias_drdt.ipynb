{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this file the process to compute the reservoir state derivative of an Echo State Netowrk is\n",
    "# defined as a cell for a tensorflow Recurrent Neural Netowrk\n",
    "\n",
    "# to pass to py_func\n",
    "def np_eigenvals(x):\n",
    "    return np.linalg.eigvals(x).astype('complex128')\n",
    "\n",
    "# Definition of the EchoState NN as a child of RNNCell of tensorflow\n",
    "# It generates the states of the reservoir; there is no output matrix\n",
    "class EchoStateRNNCelldrdt(tf.keras.layers.AbstractRNNCell):\n",
    "\n",
    "    def __init__(self, num_units, num_inputs=1, decay=0.1, rho=0.6,\n",
    "                 sparseness=0.0,\n",
    "                 sigma_in=1.0,\n",
    "                 b_in=1.0,\n",
    "                 rng=None,\n",
    "                 activation=None,\n",
    "                 reuse = False,\n",
    "                 win=None,\n",
    "                 wecho=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_units: int, Number of units in the ESN cell.\n",
    "            num_inputs: int, The number of inputs to the RNN cell.\n",
    "            decay: float, Decay/leaking of the ODE of each unit.\n",
    "            rho: float, Target spectral radius 1.\n",
    "            sparseness: float [0,1], sparseness of the inner weight matrix.\n",
    "            rng: np.random.RandomState, random number generator. (to be able to have always the same random matrix...)\n",
    "            activation: Nonlinearity to use.\n",
    "            reuse: if reusing existing matrix\n",
    "            win: input weights matrix\n",
    "            wecho: echo state matrix\n",
    "        \"\"\"\n",
    "\n",
    "        # Basic RNNCell initialization (see tensorflow documentation)\n",
    "        super(EchoStateRNNCelldrdt, self).__init__() # use the initializer of RNNCell (i.e. defines a bunch of standard variables)\n",
    "        \n",
    "        # hyperparameters\n",
    "        self._num_units  = num_units\n",
    "        self._activation = activation\n",
    "        self._num_inputs = num_inputs\n",
    "        self.decay = decay\n",
    "        self.rho = rho\n",
    "        self.sparseness = sparseness\n",
    "        self.sigma_in = sigma_in\n",
    "        self.b_in  = np.reshape(b_in, [1,1])\n",
    "        \n",
    "        # Random number generator initialization\n",
    "        self.rng = rng\n",
    "        if rng is None:\n",
    "            self.rng = np.random.RandomState()\n",
    "        \n",
    "        # build initializers for tensorflow variables\n",
    "        if (reuse == False):\n",
    "            self.win = self.buildInputMatrix()\n",
    "            self.wecho = self.buildEchoMatrix()\n",
    "        else: #in case they are passed as an argument\n",
    "            self.win = win.astype('float64')\n",
    "            self.wecho = wecho.astype('float64')\n",
    "        \n",
    "        # convert the weight to tf variable\n",
    "        self.Win   = tf.Variable(self.win, name='Win', trainable=False) \n",
    "        self.Wecho = tf.Variable(self.wecho, name='Wecho', trainable=False)\n",
    "\n",
    "        self.setEchoStateProperty()\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_units*2\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units*2\n",
    "    \n",
    "    # function that has to be implemented in the tensorflow framework\n",
    "    # return the output and new state of the RNN for given inputs and current state of the RNN\n",
    "    # state and output coincide in the reservoir\n",
    "    def call(self, inputs, state):\n",
    "        \"\"\" Reservoir state and reservoir state evolution: \n",
    "            x(i+1) = f(W*inp(i) + U*g(x(i))).\n",
    "            dx_dt(i+1) = dx(i+1)_d(inp(i))*d(inp(i))_dt + dx(i+1)_d(x(i))*d(x(i))_dt\n",
    "        \"\"\"\n",
    "        \n",
    "        # separating input and its derivative\n",
    "        x     = inputs[:,:self._num_inputs]\n",
    "        x_der = inputs[:,self._num_inputs:]\n",
    "        # separating state and its derivative\n",
    "        s     = state[0][:,:self._num_units]\n",
    "        s_der = state[0][:,self._num_units:]\n",
    "        \n",
    "        #state at next time step\n",
    "        new_state = self._activation(\n",
    "                    tf.matmul(tf.concat([x,self.b_in],axis=1),self.Win) +\n",
    "                    tf.matmul(s, self.Wecho))\n",
    "        \n",
    "        # initialize state derivative at next time step\n",
    "        deriv     = tf.TensorArray(\n",
    "                    tf.float64, self._num_units, dynamic_size=False, element_shape=[1,1])\n",
    "        \n",
    "        # Compute the derivative: dr2_dt = dr2_dx * dx_dt + dr2_dr1 * dr1_dt\n",
    "        # dx_dt is x_der and dr1_dt is s_der\n",
    "        # need to use a for loop with tf.gradients since tf.jacobian does not work in graph mode for RNN as of now (even with tape.gradient)\n",
    "        # k = 0\n",
    "        for j in tf.range(self._num_units):\n",
    "            drj_dt   = tf.reshape(tf.reduce_sum(tf.multiply(tf.gradients(new_state[0,j],x)[0],\n",
    "                                                                  x_der)), [1,1]) + \\\n",
    "                       tf.reshape(tf.reduce_sum(tf.multiply(tf.gradients(new_state[0,j],s)[0],\n",
    "                                                                  s_der)), [1,1])\n",
    "            deriv    = deriv.write(j,drj_dt)\n",
    "#             k       += 1\n",
    "            \n",
    "\n",
    "        new_state = tf.concat([new_state, tf.reshape(deriv.stack()[:,0],[1,self._num_units])],\n",
    "                              axis=1)\n",
    "        \n",
    "        return new_state, new_state   \n",
    "    \n",
    "    def setEchoStateProperty(self):\n",
    "        \"\"\" optimize U to obtain alpha-improved echo-state property \"\"\"\n",
    "        self.Wecho = self.normalizeEchoStateWeights(self.Wecho)\n",
    "        \n",
    "\n",
    "    # construct the Win matrix (dimension num_inputs x num_units)\n",
    "    def buildInputMatrix(self):\n",
    "        \"\"\"            \n",
    "            Returns:\n",
    "            \n",
    "            Matrix representing the \n",
    "            input weights to an ESN    \n",
    "        \"\"\"  \n",
    "\n",
    "        # Each unit is connected randomly to a given input with a weight from a \n",
    "        # uniform distribution between +- sigma_in (input scaling)\n",
    "        # +1 for added input bias in the input matrix\n",
    "        W = np.zeros((self._num_inputs+1,self._num_units))\n",
    "        for i in range(self._num_units):\n",
    "            W[self.rng.randint(0,self._num_inputs+1),i] = \\\n",
    "            self.rng.uniform(-self.sigma_in,self.sigma_in)\n",
    "                    \n",
    "        return W.astype('float64')\n",
    "    \n",
    "    def getInputMatrix(self):\n",
    "        return self.win\n",
    "    \n",
    "    def buildEchoMatrix(self):\n",
    "        \"\"\"            \n",
    "            Returns:\n",
    "            \n",
    "            Matrix representing the \n",
    "            inner weights to an ESN    \n",
    "        \"\"\"    \n",
    "        \n",
    "        # Build random matrix from uniform distribution\n",
    "        W = self.rng.uniform(-1.0,1.0, [self._num_units, self._num_units]).astype(\"float64\") * \\\n",
    "                (self.rng.rand(self._num_units, self._num_units) < (1. - self.sparseness) ) # trick to add zeros to have the sparseness required\n",
    "        return W\n",
    "    \n",
    "    def normalizeEchoStateWeights(self, W):\n",
    "        # Sets the spectral radius rho\n",
    "\n",
    "        eigvals = tf.py_function(np_eigenvals, [W], tf.complex128)\n",
    "        W = W / tf.reduce_max(tf.abs(eigvals))*self.rho #imposes spectral radius\n",
    "\n",
    "        return W\n",
    "    \n",
    "    def getEchoMatrix(self):\n",
    "        return self.wecho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NAKD\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# import h5py\n",
    "\n",
    "# # to pass to py_func\n",
    "# def np_eigenvals(x):\n",
    "#     return np.linalg.eigvals(x).astype('complex128')\n",
    "\n",
    "# # Definition of the EchoState NN as a child of RNNCell of tensorflow\n",
    "# class EchoStateRNNCelldrdt(tf.keras.layers.AbstractRNNCell):\n",
    "\n",
    "#     def __init__(self, num_units, num_inputs=1, decay=0.1, rho=0.6,\n",
    "#                  sparseness=0.0,\n",
    "#                  sigma_in=1.0,\n",
    "#                  b_in=1.0,\n",
    "#                  rng=None,\n",
    "#                  activation=None,\n",
    "#                  reuse = False,\n",
    "#                  win=None,\n",
    "#                  wecho=None):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             num_units: int, Number of units in the ESN cell.\n",
    "#             num_inputs: int, The number of inputs to the RNN cell.\n",
    "#             decay: float, Decay/leaking of the ODE of each unit.\n",
    "#             rho: float, Target spectral radius 1.\n",
    "#             sparseness: float [0,1], sparseness of the inner weight matrix.\n",
    "#             rng: np.random.RandomState, random number generator. (to be able to have always the same random matrix...)\n",
    "#             activation: Nonlinearity to use.\n",
    "#             reuse: if reusing existing matrix\n",
    "#             win: input weights matrix\n",
    "#             wecho: echo state matrix\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Basic RNNCell initialization (see tensorflow documentation)\n",
    "#         super(EchoStateRNNCelldrdt, self).__init__() # use the initializer of RNNCell (i.e. defines a bunch of standard variables)\n",
    "        \n",
    "#         # fixed variables\n",
    "#         self._num_units  = num_units\n",
    "#         self._activation = activation\n",
    "#         self._num_inputs = num_inputs\n",
    "        \n",
    "#         # variables that potentially can be changed/optimized (for future version)\n",
    "#         self.decay = decay\n",
    "#         self.rho = rho # rho has to be <1. to ensure the echo state property (see [2])\n",
    "#         self.sparseness = sparseness\n",
    "#         self.sigma_in = sigma_in\n",
    "#         self.b_in  = np.reshape(b_in, [1,1])\n",
    "        \n",
    "#         # Random number generator initialization\n",
    "#         self.rng = rng\n",
    "#         if rng is None:\n",
    "#             self.rng = np.random.RandomState()\n",
    "        \n",
    "#         # build initializers for tensorflow variables\n",
    "#         if (reuse == False):\n",
    "#             self.win = self.buildInputMatrix()\n",
    "#             self.wecho = self.buildEchoMatrix()\n",
    "#         else:\n",
    "#             self.win = win.astype('float64')\n",
    "#             self.wecho = wecho.astype('float64')\n",
    "        \n",
    "#         # convert the weight to tf variable\n",
    "#         self.Win   = tf.Variable(self.win, name='Win', trainable=False) \n",
    "#         self.Wecho = tf.Variable(self.wecho, name='Wecho', trainable=False)\n",
    "\n",
    "#         self.setEchoStateProperty()\n",
    "\n",
    "#     @property\n",
    "#     def state_size(self):\n",
    "#         return self._num_units*2\n",
    "\n",
    "#     @property\n",
    "#     def output_size(self):\n",
    "#         return self._num_units*2\n",
    "\n",
    "\n",
    "#     # function that has to be implemented in the tensorflow framework\n",
    "#     # receives input and previous state and returns state and output (which are the same for the ESN without Wout)\n",
    "#     def call(self, inputs, state):\n",
    "#         \"\"\" Compute the reservoir state and its derivative \"\"\"\n",
    "\n",
    "#         # separating input and its derivative\n",
    "#         x     = inputs[:,:self._num_inputs]\n",
    "#         x_der = inputs[:,self._num_inputs:]\n",
    "#         # separating state and its derivative\n",
    "#         s     = state[0][:,:self._num_units]\n",
    "#         s_der = state[0][:,self._num_units:]\n",
    "        \n",
    "#         #state at next time step\n",
    "#         new_state = self._activation(\n",
    "#                     tf.matmul(tf.concat([x,self.b_in],axis=1),self.Win) +\n",
    "#                     tf.matmul(s, self.Wecho))\n",
    "        \n",
    "#         # initialize state derivative at next time step\n",
    "#         deriv     = tf.TensorArray(\n",
    "#                     tf.float64, self._num_units, dynamic_size=False, element_shape=[1,1])\n",
    "        \n",
    "#         # Compute the derivative: dr2_dt = dr2_dx * dx_dt + dr2_dr1 * dr1_dt\n",
    "#         # dx_dt is x_der and dr1_dt is s_der\n",
    "#         # need to use a for loop with tf.gradients since tf.jacobian does not work in graph mode for RNN as of now (even with tape.gradient)\n",
    "#         # k = 0\n",
    "#         for j in tf.range(self._num_units):\n",
    "#             drj_dt   = tf.reshape(tf.reduce_sum(tf.multiply(tf.gradients(new_state[0,j],x)[0],\n",
    "#                                                                   x_der)), [1,1]) + \\\n",
    "#                        tf.reshape(tf.reduce_sum(tf.multiply(tf.gradients(new_state[0,j],s)[0],\n",
    "#                                                                   s_der)), [1,1])\n",
    "#             deriv    = deriv.write(j,drj_dt)\n",
    "# #             k       += 1\n",
    "            \n",
    "\n",
    "#         new_state = tf.concat([new_state, tf.reshape(deriv.stack()[:,0],[1,self._num_units])],\n",
    "#                               axis=1)\n",
    "        \n",
    "#         return new_state, new_state   \n",
    "    \n",
    "#     def setEchoStateProperty(self):\n",
    "#         \"\"\" optimize U to obtain alpha-improved echo-state property \"\"\"\n",
    "#         # I know it's stupid for the time being but it is a placeholder for future treatment of the matrix\n",
    "#         # (potential meta-optimization and other)\n",
    "#         self.Wecho = self.normalizeEchoStateWeights(self.Wecho)\n",
    "        \n",
    "\n",
    "#     # construct the Win matrix (dimension num_inputs x num_units)\n",
    "#     def buildInputMatrix(self):\n",
    "#         \"\"\"            \n",
    "#             Returns:\n",
    "            \n",
    "#             Matrix representing the \n",
    "#             input weights to an ESN    \n",
    "#         \"\"\"  \n",
    "\n",
    "#         # Input weight matrix initializer according to [3,4]\n",
    "#         # Each unit is connected randomly to a given input with a weight from a uniform distribution\n",
    "        \n",
    "#         # without bias at the input\n",
    "#         #W = np.zeros((self._num_inputs,self._num_units))\n",
    "#         #for i in range(self._num_units):\n",
    "#             #W[self.rng.randint(0,self._num_inputs),i] = self.rng.uniform(-self.sigma_in,self.sigma_in)\n",
    "        \n",
    "#         # Added bias in the input matrix\n",
    "#         W = np.zeros((self._num_inputs+1,self._num_units))\n",
    "#         for i in range(self._num_units):\n",
    "#             W[self.rng.randint(0,self._num_inputs+1),i] = \\\n",
    "#             self.rng.uniform(-self.sigma_in,self.sigma_in)\n",
    "            \n",
    "#         # Dense input weigth [input] as in [1,2]\n",
    "#         # Input weigth matrix [input]\n",
    "#         #W = self.rng.uniform(-self.sigma_in, self.sigma_in, [self.num_inputs, self._num_units]).astype(\"float64\")\n",
    "        \n",
    "#         # Dense input weigth [bias, input] (as in [1,2])\n",
    "#         #W = self.rng.uniform(-self.sigma_in, self.sigma_in, [self.num_inputs+1, self._num_units]).astype(\"float64\")\n",
    "        \n",
    "#         return W.astype('float64')\n",
    "    \n",
    "#     def getInputMatrix(self):\n",
    "#         return self.win\n",
    "    \n",
    "#     def buildEchoMatrix(self):\n",
    "#         \"\"\"            \n",
    "#             Returns:\n",
    "            \n",
    "#             A 1-D tensor representing the \n",
    "#             inner weights to an ESN (to be optimized)        \n",
    "#         \"\"\"    \n",
    "\n",
    "#         # Inner weight tensor initializer\n",
    "#         # 1) Build random matrix from normal distribution between [0.,1.]\n",
    "#         #W = self.rng.randn(self._num_units, self._num_units).astype(\"float64\") * \\\n",
    "#                 #(self.rng.rand(self._num_units, self._num_units) < (1. - self.sparseness) )\n",
    "        \n",
    "#         # 2) Build random matrix from uniform distribution\n",
    "#         W = self.rng.uniform(-1.0,1.0, [self._num_units, self._num_units]).astype(\"float64\") * \\\n",
    "#                 (self.rng.rand(self._num_units, self._num_units) < (1. - self.sparseness) ) # trick to add zeros to have the sparseness required\n",
    "#         return W\n",
    "    \n",
    "#     def normalizeEchoStateWeights(self, W):\n",
    "#         # Normalize to spectral radius rho\n",
    "\n",
    "#         eigvals = tf.py_function(np_eigenvals, [W], tf.complex128)\n",
    "#         W = W / tf.reduce_max(tf.abs(eigvals))*self.rho # sufficient conditions to ensure that the spectral radius is rho\n",
    "\n",
    "#         return W\n",
    "    \n",
    "#     def getEchoMatrix(self):\n",
    "#         return self.wecho\n",
    "\n",
    "# #     def save_ESN(self,fln,Wout=None,CurrState=None,lmb=None):\n",
    "        \n",
    "# #         hf = h5py.File(fln,'w')\n",
    "        \n",
    "# #         hf.create_dataset('Win',data=self.getInputMatrix() )\n",
    "# #         hf.create_dataset('Wecho',data=self.getEchoMatrix() )\n",
    "# #         hf.create_dataset('Wout',data=Wout)\n",
    "        \n",
    "# #         hf.create_dataset('rho',data=self.rho)\n",
    "# #         hf.create_dataset('num_inputs',data=self._num_inputs)\n",
    "# #         hf.create_dataset('num_units',data=self._num_units)\n",
    "# #         hf.create_dataset('sigma_in',data=self.sigma_in)\n",
    "# #         hf.create_dataset('sparseness',data=self.sparseness)\n",
    "# #         hf.create_dataset('decay',data=self.decay)\n",
    "# #         hf.create_dataset('lmb',data=lmb)\n",
    "# #         hf.create_dataset('state',data=CurrState)\n",
    "        \n",
    "# #         hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
